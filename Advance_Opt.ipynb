{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtCqv52ap3BN7C3mPuOtTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeslayr/Neural_Network_Optimization/blob/main/Advance_Opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gwFux1yKRC_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing CNN hyperparameters using GA and PSO**"
      ],
      "metadata": {
        "id": "pgY_dVhGE8y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline CNN"
      ],
      "metadata": {
        "id": "o6m4g1XLu437"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports & Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Step 2: Data Loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ7yOBC_OUcJ",
        "outputId": "a33742c1-35fd-4b19-842b-1bc2c1d7caf3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define CNN Model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # output: (32, 28, 28)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: (32, 14, 14)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # output: (64, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: (64, 7, 7)\n",
        "\n",
        "            nn.Flatten(),  # output: (64*7*7,)\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = FashionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ZfFBrqU8uR3N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loss_total += loss.item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    avg_loss = loss_total / len(dataloader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "Niy633sGuaxu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJfdeGYGubs_",
        "outputId": "97ab6bbc-076d-422d-d37e-7e28aa5a0e29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 408.0778, Accuracy: 84.32%\n",
            "Epoch [2/5], Loss: 263.7032, Accuracy: 89.76%\n",
            "Epoch [3/5], Loss: 218.7325, Accuracy: 91.42%\n",
            "Epoch [4/5], Loss: 189.3577, Accuracy: 92.58%\n",
            "Epoch [5/5], Loss: 167.2148, Accuracy: 93.36%\n",
            "Test Loss: 0.2429, Test Accuracy: 91.36%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24288778877846753, 91.36)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GA on CNN  "
      ],
      "metadata": {
        "id": "gC2Awl1JvD5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# GPU Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "val_size = 5000\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [len(train_dataset)-val_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "# CNN Model Builder\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, conv1_out, conv2_out, fc_units):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten_size = (conv2_out * 7 * 7)\n",
        "        self.fc1 = nn.Linear(self.flatten_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# GA Hyperparameters\n",
        "POP_SIZE = 20\n",
        "GENERATIONS = 10\n",
        "TOP_K = 4\n",
        "\n",
        "param_bounds = {\n",
        "    \"lr\": [1e-4, 1e-2],\n",
        "    \"conv1_out\": [16, 64],\n",
        "    \"conv2_out\": [32, 128],\n",
        "    \"fc_units\": [64, 256],\n",
        "}\n",
        "\n",
        "def random_gene():\n",
        "    return {\n",
        "        \"lr\": round(10 ** random.uniform(np.log10(param_bounds['lr'][0]), np.log10(param_bounds['lr'][1])), 5),\n",
        "        \"conv1_out\": random.randint(*param_bounds['conv1_out']),\n",
        "        \"conv2_out\": random.randint(*param_bounds['conv2_out']),\n",
        "        \"fc_units\": random.randint(*param_bounds['fc_units'])\n",
        "    }\n",
        "\n",
        "def mutate(gene):\n",
        "    mutated = deepcopy(gene)\n",
        "    key = random.choice(list(param_bounds.keys()))\n",
        "    return random_gene() if random.random() < 0.3 else gene\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    child = {}\n",
        "    for key in param_bounds:\n",
        "        child[key] = parent1[key] if random.random() < 0.5 else parent2[key]\n",
        "    return child\n",
        "\n",
        "# Fitness Evaluation\n",
        "def evaluate(gene):\n",
        "    model = CNN(gene['conv1_out'], gene['conv2_out'], gene['fc_units']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=gene['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1):  # train only 1 epoch\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    return val_loss, accuracy\n",
        "\n",
        "# Genetic Algorithm Core\n",
        "def run_ga():\n",
        "    population = [random_gene() for _ in range(POP_SIZE)]\n",
        "\n",
        "    # Create history storage\n",
        "    ga_history = []\n",
        "\n",
        "    best_overall = None\n",
        "    best_loss = float('inf')\n",
        "    best_acc = 0.0\n",
        "    best_gen = -1\n",
        "\n",
        "    for gen in range(GENERATIONS):\n",
        "        # Evaluate all individuals\n",
        "        scored = []\n",
        "        for gene in population:\n",
        "            loss, acc = evaluate(gene)\n",
        "            scored.append((gene, loss, acc))  # <-- MODIFIED TO STORE TUPLE\n",
        "\n",
        "        scored.sort(key=lambda x: x[1])  # sort by loss\n",
        "\n",
        "        best_gene, loss, acc = scored[0]\n",
        "\n",
        "        # Track global best\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_acc = acc\n",
        "            best_overall = best_gene\n",
        "            best_gen = gen\n",
        "\n",
        "        print(f\"Gen {gen}: Best Loss = {loss:.4f}, Acc = {acc:.4f}\")\n",
        "        print(f\"         Best Hyperparams @ Gen {gen}: {best_gene}\")\n",
        "\n",
        "        # ======== STORE GENERATION HISTORY ======== <-- ADD THIS BLOCK\n",
        "        gen_data = {\n",
        "            'generation': gen,\n",
        "            'population': [gene for gene, _, _ in scored],\n",
        "            'fitness': [(loss, acc) for _, loss, acc in scored],\n",
        "            'best_gene': best_gene,\n",
        "            'best_loss': loss,\n",
        "            'best_acc': acc\n",
        "        }\n",
        "        ga_history.append(gen_data)\n",
        "        # ======== END HISTORY STORAGE ========\n",
        "\n",
        "        # Genetic operations\n",
        "        top_genes = [x[0] for x in scored[:TOP_K]]\n",
        "        new_pop = top_genes.copy()\n",
        "\n",
        "        while len(new_pop) < POP_SIZE:\n",
        "            p1, p2 = random.sample(top_genes, 2)\n",
        "            child = crossover(p1, p2)\n",
        "            child = mutate(child)\n",
        "            new_pop.append(child)\n",
        "\n",
        "        population = new_pop\n",
        "\n",
        "    print(f\"\\n✅ Best hyperparameters found at generation {best_gen}: {best_overall}\")\n",
        "    print(f\"   Final Best Loss = {best_loss:.4f}, Accuracy = {best_acc:.4f}\")\n",
        "    return best_overall, ga_history\n",
        "\n",
        "# Run GA\n",
        "best_hyperparams, ga_history = run_ga()\n",
        "print(\"\\nBest hyperparameters found:\", best_hyperparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "7OT1FPIeuh4j",
        "outputId": "0663b9ad-3827-4a27-8515-2ee91e41d9b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-4260260563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# Run GA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mga_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBest hyperparameters found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-4260260563.py\u001b[0m in \u001b[0;36mrun_ga\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mscored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgene\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mscored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- MODIFIED TO STORE TUPLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-4260260563.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(gene)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Official Test Dataset\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Evaluate Best Hyperparams on Test Set\n",
        "def train_and_test(hparams, epochs=5):\n",
        "    model = CNN(hparams['conv1_out'], hparams['conv2_out'], hparams['fc_units']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"\\n🔁 Training final model for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    test_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f\"\\n📊 Final Evaluation on Test Set:\")\n",
        "    print(f\"   Test Loss     = {test_loss:.4f}\")\n",
        "    print(f\"   Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "train_and_test(best_hyperparams)"
      ],
      "metadata": {
        "id": "T5ta--z4_sZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PSO on CNN"
      ],
      "metadata": {
        "id": "7AOpLOmo02Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# ------------------------------\n",
        "# Device Setup\n",
        "# ------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------------------\n",
        "# Dataset (Subset for Speed)\n",
        "# ------------------------------\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "val_size = 5000\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [len(train_dataset) - val_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test Set (Official Evaluation)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ------------------------------\n",
        "# CNN Definition\n",
        "# ------------------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, conv1_out, conv2_out, fc_units):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten_size = conv2_out * 7 * 7\n",
        "        self.fc1 = nn.Linear(self.flatten_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ------------------------------\n",
        "# Hyperparameter Bounds\n",
        "# ------------------------------\n",
        "param_bounds = {\n",
        "    'lr': [1e-4, 1e-2],\n",
        "    'conv1_out': [16, 64],\n",
        "    'conv2_out': [32, 128],\n",
        "    'fc_units': [64, 256]\n",
        "}\n",
        "\n",
        "# ------------------------------\n",
        "# Fitness Function\n",
        "# ------------------------------\n",
        "def evaluate(hparams):\n",
        "    model = CNN(int(hparams['conv1_out']), int(hparams['conv2_out']), int(hparams['fc_units'])).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx >= 9:  # train on 10 mini-batches\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    return val_loss, accuracy\n",
        "\n",
        "# ------------------------------\n",
        "# PSO Core\n",
        "# ------------------------------\n",
        "def run_pso(particles=10, generations=20, w=0.5, c1=1.5, c2=1.5):\n",
        "    def init_particle():\n",
        "        return {\n",
        "            'lr': 10 ** random.uniform(np.log10(param_bounds['lr'][0]), np.log10(param_bounds['lr'][1])),\n",
        "            'conv1_out': random.randint(*param_bounds['conv1_out']),\n",
        "            'conv2_out': random.randint(*param_bounds['conv2_out']),\n",
        "            'fc_units': random.randint(*param_bounds['fc_units'])\n",
        "        }\n",
        "\n",
        "    def clip(hp):\n",
        "        hp['lr'] = float(np.clip(hp['lr'], *param_bounds['lr']))\n",
        "        hp['conv1_out'] = int(np.clip(round(hp['conv1_out']), *param_bounds['conv1_out']))\n",
        "        hp['conv2_out'] = int(np.clip(round(hp['conv2_out']), *param_bounds['conv2_out']))\n",
        "        hp['fc_units'] = int(np.clip(round(hp['fc_units']), *param_bounds['fc_units']))\n",
        "        return hp\n",
        "\n",
        "    population = [init_particle() for _ in range(particles)]\n",
        "    velocity = [{k: 0.0 for k in p} for p in population]\n",
        "\n",
        "    personal_best = deepcopy(population)\n",
        "    personal_best_scores = []\n",
        "    personal_best_accs = []\n",
        "\n",
        "    for p in personal_best:\n",
        "        loss, acc = evaluate(p)\n",
        "        personal_best_scores.append(loss)\n",
        "        personal_best_accs.append(acc)\n",
        "\n",
        "    global_best_idx = np.argmin(personal_best_scores)\n",
        "    global_best = deepcopy(personal_best[global_best_idx])\n",
        "    global_best_loss = personal_best_scores[global_best_idx]\n",
        "    global_best_acc = personal_best_accs[global_best_idx]\n",
        "\n",
        "    pso_history = []\n",
        "\n",
        "    for gen in range(generations):\n",
        "        for i in range(particles):\n",
        "            new_vel = {}\n",
        "            new_pos = {}\n",
        "            for key in population[i]:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                vel = (w * velocity[i][key]\n",
        "                       + c1 * r1 * (personal_best[i][key] - population[i][key])\n",
        "                       + c2 * r2 * (global_best[key] - population[i][key]))\n",
        "                new_vel[key] = vel\n",
        "                new_pos[key] = population[i][key] + vel\n",
        "            velocity[i] = new_vel\n",
        "            new_pos = clip(new_pos)\n",
        "            population[i] = new_pos\n",
        "\n",
        "            loss, acc = evaluate(new_pos)\n",
        "            if loss < personal_best_scores[i]:\n",
        "                personal_best[i] = deepcopy(new_pos)\n",
        "                personal_best_scores[i] = loss\n",
        "                personal_best_accs[i] = acc\n",
        "\n",
        "        global_best_idx = np.argmin(personal_best_scores)\n",
        "        global_best = deepcopy(personal_best[global_best_idx])\n",
        "        global_best_loss = personal_best_scores[global_best_idx]\n",
        "        global_best_acc = personal_best_accs[global_best_idx]\n",
        "\n",
        "        print(f\"Gen {gen}: Best Loss = {global_best_loss:.4f}, Acc = {global_best_acc:.4f}\")\n",
        "\n",
        "        # ======== STORE ITERATION HISTORY ======== <-- ADD THIS BLOCK\n",
        "        gen_data = {\n",
        "            'iteration': gen,\n",
        "            'swarm': deepcopy(population),\n",
        "            'fitness': [(personal_best_scores[i], personal_best_accs[i])\n",
        "                        for i in range(len(population))],\n",
        "            'global_best': deepcopy(global_best),\n",
        "            'global_loss': global_best_loss,\n",
        "            'global_acc': global_best_acc\n",
        "        }\n",
        "        pso_history.append(gen_data)\n",
        "        # ======== END HISTORY STORAGE ========\n",
        "\n",
        "    return global_best, pso_history\n",
        "\n",
        "# ------------------------------\n",
        "# Run PSO\n",
        "# ------------------------------\n",
        "best_pso_hyperparams, pso_history = run_pso()\n",
        "print(\"\\nBest hyperparameters from PSO:\", best_pso_hyperparams)\n",
        "\n",
        "# ------------------------------\n",
        "# Final Full Training & Test Evaluation\n",
        "# ------------------------------\n",
        "def train_and_test(hparams, epochs=5):\n",
        "    model = CNN(hparams['conv1_out'], hparams['conv2_out'], hparams['fc_units']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "train_and_test(best_pso_hyperparams)"
      ],
      "metadata": {
        "id": "YaPSAbMk04U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plotting of GA vs PSO**"
      ],
      "metadata": {
        "id": "onRIk2DOg9vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================================\n",
        "# PLOTTING FUNCTIONS\n",
        "# ==================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from pandas.plotting import parallel_coordinates\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Normalization helper\n",
        "def normalize_param(value, param_name):\n",
        "    bounds = param_bounds[param_name]\n",
        "    if param_name == 'lr':\n",
        "        log_val = np.log10(value)\n",
        "        log_min = np.log10(bounds[0])\n",
        "        log_max = np.log10(bounds[1])\n",
        "        return (log_val - log_min) / (log_max - log_min)\n",
        "    return (value - bounds[0]) / (bounds[1] - bounds[0])"
      ],
      "metadata": {
        "id": "e8PMtfcsucH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CONVERGENCE PLOT\n",
        "def plot_convergence(ga_history, pso_history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # GA data\n",
        "    ga_loss = [gen['best_loss'] for gen in ga_history]\n",
        "    ga_acc = [gen['best_acc'] for gen in ga_history]\n",
        "\n",
        "    # PSO data\n",
        "    pso_loss = [gen['global_loss'] for gen in pso_history]\n",
        "    pso_acc = [gen['global_acc'] for gen in pso_history]\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(ga_loss, 'b-o', label='GA')\n",
        "    plt.plot(pso_loss, 'r-s', label='PSO')\n",
        "    plt.xlabel('Generation/Iteration')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.title('Loss Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(ga_acc, 'b-o', label='GA')\n",
        "    plt.plot(pso_acc, 'r-s', label='PSO')\n",
        "    plt.xlabel('Generation/Iteration')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.title('Accuracy Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('convergence.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_convergence(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "fKre15qIudNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 3D TRAJECTORY ANIMATION\n",
        "def create_3d_animation(ga_history, pso_history):\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Prepare data containers\n",
        "    ga_points = []\n",
        "    pso_points = []\n",
        "\n",
        "    # Create animation function\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "\n",
        "        # GA points\n",
        "        for gen in range(min(frame+1, len(ga_history))):\n",
        "            for gene in ga_history[gen]['population']:\n",
        "                x = normalize_param(gene['lr'], 'lr')\n",
        "                y = normalize_param(gene['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(gene['conv2_out'], 'conv2_out')\n",
        "                ga_points.append((x, y, z))\n",
        "\n",
        "        # PSO points\n",
        "        for iter in range(min(frame+1, len(pso_history))):\n",
        "            for particle in pso_history[iter]['swarm']:\n",
        "                x = normalize_param(particle['lr'], 'lr')\n",
        "                y = normalize_param(particle['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(particle['conv2_out'], 'conv2_out')\n",
        "                pso_points.append((x, y, z))\n",
        "\n",
        "        # Convert to arrays\n",
        "        if ga_points:\n",
        "            ga_arr = np.array(ga_points)\n",
        "            ax.scatter(ga_arr[:,0], ga_arr[:,1], ga_arr[:,2],\n",
        "                      c='blue', alpha=0.3, s=10, label='GA')\n",
        "\n",
        "        if pso_points:\n",
        "            pso_arr = np.array(pso_points)\n",
        "            ax.scatter(pso_arr[:,0], pso_arr[:,1], pso_arr[:,2],\n",
        "                      c='red', alpha=0.3, s=10, marker='s', label='PSO')\n",
        "\n",
        "        # Plot best trajectories\n",
        "        if ga_history:\n",
        "            ga_best = []\n",
        "            for gen in range(min(frame+1, len(ga_history))):\n",
        "                gene = ga_history[gen]['best_gene']\n",
        "                x = normalize_param(gene['lr'], 'lr')\n",
        "                y = normalize_param(gene['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(gene['conv2_out'], 'conv2_out')\n",
        "                ga_best.append((x, y, z))\n",
        "            ga_best_arr = np.array(ga_best)\n",
        "            ax.plot(ga_best_arr[:,0], ga_best_arr[:,1], ga_best_arr[:,2],\n",
        "                   'b-', linewidth=2, label='GA Best')\n",
        "            ax.scatter(ga_best_arr[-1,0], ga_best_arr[-1,1], ga_best_arr[-1,2],\n",
        "                      c='gold', s=100, edgecolor='black', label='Current Best')\n",
        "\n",
        "        if pso_history:\n",
        "            pso_best = []\n",
        "            for iter in range(min(frame+1, len(pso_history))):\n",
        "                particle = pso_history[iter]['global_best']\n",
        "                x = normalize_param(particle['lr'], 'lr')\n",
        "                y = normalize_param(particle['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(particle['conv2_out'], 'conv2_out')\n",
        "                pso_best.append((x, y, z))\n",
        "            pso_best_arr = np.array(pso_best)\n",
        "            ax.plot(pso_best_arr[:,0], pso_best_arr[:,1], pso_best_arr[:,2],\n",
        "                   'r-', linewidth=2, label='PSO Best')\n",
        "\n",
        "        # Set labels and title\n",
        "        ax.set_xlabel('log10(Learning Rate)')\n",
        "        ax.set_ylabel('conv1_out')\n",
        "        ax.set_zlabel('conv2_out')\n",
        "        ax.set_title(f'Hyperparameter Optimization Trajectories\\nGeneration/Iteration: {frame}')\n",
        "        ax.legend()\n",
        "\n",
        "        # Set viewing angle\n",
        "        ax.view_init(30, frame * 2)\n",
        "\n",
        "    # Create animation\n",
        "    ani = FuncAnimation(fig, update, frames=max(len(ga_history), len(pso_history)),\n",
        "                        interval=500, blit=False)\n",
        "\n",
        "    # Save animation\n",
        "    ani.save('optimization_trajectory.mp4', writer='ffmpeg', fps=2, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "create_3d_animation(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "obPtWePVuqun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. PARALLEL COORDINATES PLOT\n",
        "def plot_parallel_coordinates(ga_history, pso_history):\n",
        "    data = []\n",
        "\n",
        "    # Collect GA data\n",
        "    for gen in ga_history:\n",
        "        for gene, (loss, acc) in zip(gen['population'], gen['fitness']):\n",
        "            row = gene.copy()\n",
        "            row['loss'] = loss\n",
        "            row['accuracy'] = acc\n",
        "            row['method'] = 'GA'\n",
        "            data.append(row)\n",
        "\n",
        "    # Collect PSO data\n",
        "    for gen in pso_history:\n",
        "        for particle, (loss, acc) in zip(gen['swarm'], gen['fitness']):\n",
        "            row = particle.copy()\n",
        "            row['loss'] = loss\n",
        "            row['accuracy'] = acc\n",
        "            row['method'] = 'PSO'\n",
        "            data.append(row)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Normalize parameters for better visualization\n",
        "    for param in param_bounds.keys():\n",
        "        if param == 'lr':\n",
        "            df['log_lr'] = np.log10(df['lr'])\n",
        "            min_val = np.log10(param_bounds['lr'][0])\n",
        "            max_val = np.log10(param_bounds['lr'][1])\n",
        "            df['norm_lr'] = (df['log_lr'] - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            min_val = param_bounds[param][0]\n",
        "            max_val = param_bounds[param][1]\n",
        "            df[f'norm_{param}'] = (df[param] - min_val) / (max_val - min_val)\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    parallel_coordinates(\n",
        "        df[['method', 'norm_lr', 'norm_conv1_out', 'norm_conv2_out', 'norm_fc_units', 'accuracy']],\n",
        "        'method',\n",
        "        color=['blue', 'red'],\n",
        "        alpha=0.1\n",
        "    )\n",
        "    plt.title('Hyperparameter Space Exploration')\n",
        "    plt.xlabel('Parameters (Normalized)')\n",
        "    plt.ylabel('Value')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig('parallel_coordinates.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_parallel_coordinates(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "ZEKwtBZiurnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. HYPERPARAMETER EVOLUTION\n",
        "def plot_hyperparameter_evolution(ga_history, pso_history):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    params = ['lr', 'conv1_out', 'conv2_out', 'fc_units']\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "        ax = axs[i//2, i%2]\n",
        "\n",
        "        # GA evolution\n",
        "        ga_vals = []\n",
        "        for gen in ga_history:\n",
        "            gen_vals = [gene[param] for gene in gen['population']]\n",
        "            ga_vals.append(gen_vals)\n",
        "        ga_vals = np.array(ga_vals)\n",
        "\n",
        "        # Plot GA\n",
        "        for j in range(ga_vals.shape[1]):\n",
        "            ax.plot(ga_vals[:, j], 'b-', alpha=0.1)\n",
        "        ax.plot(np.median(ga_vals, axis=1), 'b-o', linewidth=2, label='GA Median')\n",
        "\n",
        "        # PSO evolution\n",
        "        pso_vals = []\n",
        "        for gen in pso_history:\n",
        "            gen_vals = [particle[param] for particle in gen['swarm']]\n",
        "            pso_vals.append(gen_vals)\n",
        "        pso_vals = np.array(pso_vals)\n",
        "\n",
        "        # Plot PSO\n",
        "        for j in range(pso_vals.shape[1]):\n",
        "            ax.plot(pso_vals[:, j], 'r-', alpha=0.1)\n",
        "        ax.plot(np.median(pso_vals, axis=1), 'r-s', linewidth=2, label='PSO Median')\n",
        "\n",
        "        # Formatting\n",
        "        ax.set_xlabel('Generation/Iteration')\n",
        "        ax.set_ylabel(param)\n",
        "        if param == 'lr':\n",
        "            ax.set_yscale('log')\n",
        "        ax.set_title(f'{param} Evolution')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hyperparameter_evolution.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_hyperparameter_evolution(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "tdnFwGf6utnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. PERFORMANCE HEATMAP\n",
        "def plot_performance_heatmap(ga_history, pso_history):\n",
        "    # Prepare data\n",
        "    all_data = []\n",
        "\n",
        "    # GA data\n",
        "    for gen in ga_history:\n",
        "        for gene, (loss, acc) in zip(gen['population'], gen['fitness']):\n",
        "            all_data.append({\n",
        "                'method': 'GA',\n",
        "                'conv1_out': gene['conv1_out'],\n",
        "                'conv2_out': gene['conv2_out'],\n",
        "                'accuracy': acc\n",
        "            })\n",
        "\n",
        "    # PSO data\n",
        "    for gen in pso_history:\n",
        "        for particle, (loss, acc) in zip(gen['swarm'], gen['fitness']):\n",
        "            all_data.append({\n",
        "                'method': 'PSO',\n",
        "                'conv1_out': particle['conv1_out'],\n",
        "                'conv2_out': particle['conv2_out'],\n",
        "                'accuracy': acc\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Create pivot tables\n",
        "    ga_pivot = df[df['method']=='GA'].pivot_table(\n",
        "        index='conv1_out', columns='conv2_out', values='accuracy', aggfunc='mean'\n",
        "    )\n",
        "    pso_pivot = df[df['method']=='PSO'].pivot_table(\n",
        "        index='conv1_out', columns='conv2_out', values='accuracy', aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    # Plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
        "\n",
        "    # GA heatmap\n",
        "    sns.heatmap(ga_pivot, annot=True, fmt=\".3f\", cmap=\"Blues\", ax=ax1)\n",
        "    ax1.set_title('GA Accuracy Heatmap')\n",
        "    ax1.set_xlabel('conv2_out')\n",
        "    ax1.set_ylabel('conv1_out')\n",
        "\n",
        "    # PSO heatmap\n",
        "    sns.heatmap(pso_pivot, annot=True, fmt=\".3f\", cmap=\"Reds\", ax=ax2)\n",
        "    ax2.set_title('PSO Accuracy Heatmap')\n",
        "    ax2.set_xlabel('conv2_out')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_heatmap.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_performance_heatmap(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "sQJ2rpdDuv8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. INTERACTIVE 3D PLOT (PLOTLY)\n",
        "def create_interactive_3d(ga_history, pso_history):\n",
        "    # Prepare data\n",
        "    ga_data = []\n",
        "    pso_data = []\n",
        "\n",
        "    # GA points\n",
        "    for gen in ga_history:\n",
        "        for gene, (loss, acc) in zip(gen['population'], gen['fitness']):\n",
        "            ga_data.append({\n",
        "                'x': np.log10(gene['lr']),\n",
        "                'y': gene['conv1_out'],\n",
        "                'z': gene['conv2_out'],\n",
        "                'accuracy': acc,\n",
        "                'fc_units': gene['fc_units'],\n",
        "                'generation': gen['generation']\n",
        "            })\n",
        "\n",
        "    # PSO points\n",
        "    for gen in pso_history:\n",
        "        for particle, (loss, acc) in zip(gen['swarm'], gen['fitness']):\n",
        "            pso_data.append({\n",
        "                'x': np.log10(particle['lr']),\n",
        "                'y': particle['conv1_out'],\n",
        "                'z': particle['conv2_out'],\n",
        "                'accuracy': acc,\n",
        "                'fc_units': particle['fc_units'],\n",
        "                'iteration': gen['iteration']\n",
        "            })\n",
        "\n",
        "    # Create GA trace\n",
        "    ga_df = pd.DataFrame(ga_data)\n",
        "    ga_trace = go.Scatter3d(\n",
        "        x=ga_df['x'],\n",
        "        y=ga_df['y'],\n",
        "        z=ga_df['z'],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=ga_df['fc_units']/50,\n",
        "            color=ga_df['accuracy'],\n",
        "            colorscale='Blues',\n",
        "            opacity=0.7,\n",
        "            colorbar=dict(title='Accuracy')\n",
        "        ),\n",
        "        name='GA',\n",
        "        hovertext=ga_df.apply(lambda row:\n",
        "            f\"Accuracy: {row['accuracy']:.4f}<br>\"\n",
        "            f\"LR: {10**row['x']:.5f}<br>\"\n",
        "            f\"conv1: {row['y']}<br>\"\n",
        "            f\"conv2: {row['z']}<br>\"\n",
        "            f\"fc: {row['fc_units']}<br>\"\n",
        "            f\"Gen: {row['generation']}\", axis=1)\n",
        "    )\n",
        "\n",
        "    # Create PSO trace\n",
        "    pso_df = pd.DataFrame(pso_data)\n",
        "    pso_trace = go.Scatter3d(\n",
        "        x=pso_df['x'],\n",
        "        y=pso_df['y'],\n",
        "        z=pso_df['z'],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=pso_df['fc_units']/50,\n",
        "            color=pso_df['accuracy'],\n",
        "            colorscale='Reds',\n",
        "            opacity=0.7,\n",
        "            colorbar=dict(title='Accuracy')\n",
        "        ),\n",
        "        name='PSO',\n",
        "        hovertext=pso_df.apply(lambda row:\n",
        "            f\"Accuracy: {row['accuracy']:.4f}<br>\"\n",
        "            f\"LR: {10**row['x']:.5f}<br>\"\n",
        "            f\"conv1: {row['y']}<br>\"\n",
        "            f\"conv2: {row['z']}<br>\"\n",
        "            f\"fc: {row['fc_units']}<br>\"\n",
        "            f\"Iter: {row['iteration']}\", axis=1)\n",
        "    )\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure(data=[ga_trace, pso_trace])\n",
        "\n",
        "    # Set layout\n",
        "    fig.update_layout(\n",
        "        title='Hyperparameter Optimization Space',\n",
        "        scene=dict(\n",
        "            xaxis_title='log10(Learning Rate)',\n",
        "            yaxis_title='conv1_out',\n",
        "            zaxis_title='conv2_out',\n",
        "            camera=dict(\n",
        "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
        "            )\n",
        "        ),\n",
        "        width=1200,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "    # Save and show\n",
        "    fig.write_html(\"interactive_optimization.html\")\n",
        "    fig.show()\n",
        "\n",
        "create_interactive_3d(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "t7OFYCPtuygT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}