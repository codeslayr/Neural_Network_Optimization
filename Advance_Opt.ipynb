{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNICeW1SFtOyRi9e1fMCFyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeslayr/Neural_Network_Optimization/blob/main/Advance_Opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing CNN hyperparameters using SGD**"
      ],
      "metadata": {
        "id": "pddjzwhr149c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load and split FashionMNIST dataset\n",
        "full_train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [55000, 5000])\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# CNN Model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, dataloader, criterion, optimizer, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Random Search Config\n",
        "search_space = {\n",
        "    'lr': lambda: 10 ** np.random.uniform(-4, -1),\n",
        "    'momentum': lambda: np.random.uniform(0.5, 0.99),\n",
        "    'batch_size': lambda: random.choice([32, 64, 128]),\n",
        "    'weight_decay': lambda: np.random.uniform(0.0, 1e-2)\n",
        "}\n",
        "\n",
        "results = []\n",
        "best_config = None\n",
        "best_acc = 0\n",
        "best_epoch = -1\n",
        "num_trials = 20\n",
        "\n",
        "# Run random search\n",
        "for trial in range(num_trials):\n",
        "    config = {key: sampler() for key, sampler in search_space.items()}\n",
        "    batch_size = int(config['batch_size'])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = FashionCNN().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
        "\n",
        "    train(model, train_loader, criterion, optimizer, epochs=3)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    results.append((config, val_loss, val_acc))\n",
        "    print(f\"Epoch {trial+1}/{num_trials} - Acc: {val_acc:.4f}, Loss: {val_loss:.4f}, Config: {config}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_config = config\n",
        "        best_epoch = trial + 1\n",
        "\n",
        "# Final training and evaluation\n",
        "print(f\"\\nüîç Best Hyperparameter Found at epoch {best_epoch} {best_config}\")\n",
        "print(f\"Best hyperparameters from SGD: {best_config}\")\n",
        "\n",
        "final_model = FashionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(final_model.parameters(), lr=best_config['lr'], momentum=best_config['momentum'], weight_decay=best_config['weight_decay'])\n",
        "\n",
        "combined_loader = DataLoader(full_train_dataset, batch_size=int(best_config['batch_size']), shuffle=True)\n",
        "print()\n",
        "for ep in range(5):\n",
        "    train(final_model, combined_loader, criterion, optimizer, epochs=1)\n",
        "    print(f\"Epoch {ep+1} completed.\")\n",
        "\n",
        "test_loss, test_acc = evaluate(final_model, test_loader, criterion)\n",
        "print(f\"\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "4eM8tbA6IlwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plotting"
      ],
      "metadata": {
        "id": "g86fkje7InN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "import psutil\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "# 1. Convergence Progression\n",
        "def plot_convergence(results):\n",
        "    \"\"\"Plot validation accuracy and loss progression across trials\"\"\"\n",
        "    trials = range(1, len(results) + 1)\n",
        "    val_accs = [res[2] for res in results]\n",
        "    val_losses = [res[1] for res in results]\n",
        "\n",
        "    # Find best accuracy positions\n",
        "    best_accs = [max(val_accs[:i+1]) for i in range(len(val_accs))]\n",
        "    best_losses = [min(val_losses[:i+1]) for i in range(len(val_losses))]\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Accuracy plot\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Trial')\n",
        "    ax1.set_ylabel('Accuracy', color=color)\n",
        "    ax1.plot(trials, val_accs, 'o-', color='lightblue', label='Trial Accuracy')\n",
        "    ax1.plot(trials, best_accs, 's-', color=color, label='Best Accuracy')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    ax1.set_ylim(0.7, 0.95)\n",
        "    ax1.grid(alpha=0.3)\n",
        "    ax1.set_title('Convergence Progression')\n",
        "\n",
        "    # Loss plot\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(trials, val_losses, 'o-', color='salmon', label='Trial Loss')\n",
        "    ax2.plot(trials, best_losses, 's-', color=color, label='Best Loss')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    ax2.set_ylim(0.1, 1.0)\n",
        "\n",
        "    # Combined legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='lower right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('convergence_progression.png')\n",
        "    plt.close()\n",
        "\n",
        "# 2. Hyperparameter Evolution\n",
        "def plot_hyperparameter_evolution(results):\n",
        "    \"\"\"Visualize hyperparameter trajectories and correlation with accuracy\"\"\"\n",
        "    # Prepare data\n",
        "    df = pd.DataFrame({\n",
        "        'trial': range(1, len(results)+1),\n",
        "        'lr': [res[0]['lr'] for res in results],\n",
        "        'momentum': [res[0]['momentum'] for res in results],\n",
        "        'batch_size': [res[0]['batch_size'] for res in results],\n",
        "        'weight_decay': [res[0]['weight_decay'] for res in results],\n",
        "        'accuracy': [res[2] for res in results]\n",
        "    })\n",
        "\n",
        "    # Parallel coordinates plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pd.plotting.parallel_coordinates(\n",
        "        df, 'trial',\n",
        "        cols=['lr', 'momentum', 'batch_size', 'weight_decay', 'accuracy'],\n",
        "        color=plt.cm.viridis(np.linspace(0, 1, len(df)))\n",
        "    )\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title('Hyperparameter Evolution Across Trials')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('hyperparameter_evolution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Correlation matrix\n",
        "    corr = df.corr()\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title('Hyperparameter-Accuracy Correlation')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('hyperparameter_correlation.png')\n",
        "    plt.close()\n",
        "\n",
        "# 3. Search Space Exploration\n",
        "def plot_search_space_exploration(results):\n",
        "    \"\"\"Visualize hyperparameter distribution and density\"\"\"\n",
        "    # Prepare data\n",
        "    df = pd.DataFrame({\n",
        "        'lr': [res[0]['lr'] for res in results],\n",
        "        'momentum': [res[0]['momentum'] for res in results],\n",
        "        'batch_size': [res[0]['batch_size'] for res in results],\n",
        "        'weight_decay': [res[0]['weight_decay'] for res in results],\n",
        "        'accuracy': [res[2] for res in results]\n",
        "    })\n",
        "\n",
        "    # Pairwise relationships\n",
        "    g = sns.PairGrid(df, diag_sharey=False)\n",
        "    g.map_upper(sns.scatterplot, s=50)\n",
        "    g.map_lower(sns.kdeplot, fill=True)\n",
        "    g.map_diag(sns.histplot, kde=True)\n",
        "    g.fig.suptitle('Search Space Exploration', y=1.02)\n",
        "    plt.show()\n",
        "    plt.savefig('search_space_pairplot.png')\n",
        "    plt.close()\n",
        "\n",
        "    # PCA projection\n",
        "    pca = PCA(n_components=2)\n",
        "    params = df[['lr', 'momentum', 'batch_size', 'weight_decay']]\n",
        "    pca_result = pca.fit_transform(params)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(\n",
        "        pca_result[:, 0], pca_result[:, 1],\n",
        "        c=df['accuracy'], cmap='viridis', s=100, alpha=0.8\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Accuracy')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.title('Hyperparameter Space Projection (PCA)')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('search_space_pca.png')\n",
        "    plt.close()\n",
        "\n",
        "# 4. Training Dynamics (For Best Run)\n",
        "def plot_training_dynamics(model, train_loader, val_loader):\n",
        "    \"\"\"Plot training and validation metrics during training\"\"\"\n",
        "    # Track metrics during training\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=best_config['lr'],\n",
        "                          momentum=best_config['momentum'],\n",
        "                          weight_decay=best_config['weight_decay'])\n",
        "\n",
        "    for epoch in range(5):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_acc = correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Create plots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "    # Loss plot\n",
        "    epochs = range(1, 6)\n",
        "    ax1.plot(epochs, train_losses, 'b-o', label='Training Loss')\n",
        "    ax1.plot(epochs, val_losses, 'r--s', label='Validation Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(epochs, train_accs, 'g-o', label='Training Accuracy')\n",
        "    ax2.plot(epochs, val_accs, 'm--s', label='Validation Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('training_dynamics.png')\n",
        "    plt.close()\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "# 5. Computation Profile (Modified Training Loop)\n",
        "def track_computation_profile():\n",
        "    \"\"\"Track computational resources during training\"\"\"\n",
        "    # Modify training function to track resources\n",
        "    train_times = []\n",
        "    cpu_usages = []\n",
        "    mem_usages = []\n",
        "    gpu_usages = []\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "        config = {key: sampler() for key, sampler in search_space.items()}\n",
        "        batch_size = int(config['batch_size'])\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        model = FashionCNN().to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n",
        "                              momentum=config['momentum'],\n",
        "                              weight_decay=config['weight_decay'])\n",
        "\n",
        "        # Start tracking\n",
        "        start_time = time.time()\n",
        "        cpu_before = psutil.cpu_percent()\n",
        "        mem_before = psutil.virtual_memory().percent\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats(device)\n",
        "            gpu_before = torch.cuda.memory_allocated(device) / (1024 ** 2)  # MB\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        for epoch in range(3):\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # End tracking\n",
        "        train_time = time.time() - start_time\n",
        "        cpu_after = psutil.cpu_percent()\n",
        "        mem_after = psutil.virtual_memory().percent\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_after = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # MB\n",
        "        else:\n",
        "            gpu_after = 0\n",
        "\n",
        "        train_times.append(train_time)\n",
        "        cpu_usages.append((cpu_before + cpu_after) / 2)\n",
        "        mem_usages.append((mem_before + mem_after) / 2)\n",
        "        gpu_usages.append(gpu_after)\n",
        "\n",
        "        # Evaluation\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "        results.append((config, val_loss, val_acc))\n",
        "\n",
        "    # Plot computation profile\n",
        "    trials = range(1, num_trials+1)\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Time and Memory\n",
        "    ax1.set_xlabel('Trial')\n",
        "    ax1.set_ylabel('Time (s) / Memory (%)')\n",
        "    ax1.plot(trials, train_times, 'b-o', label='Training Time')\n",
        "    ax1.plot(trials, mem_usages, 'g--s', label='Memory Usage')\n",
        "    ax1.tick_params(axis='y')\n",
        "    ax1.legend(loc='upper left')\n",
        "\n",
        "    # GPU Usage\n",
        "    if torch.cuda.is_available():\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.set_ylabel('GPU Memory (MB)', color='r')\n",
        "        ax2.plot(trials, gpu_usages, 'r-^', label='GPU Memory')\n",
        "        ax2.tick_params(axis='y', labelcolor='r')\n",
        "        ax2.legend(loc='upper right')\n",
        "\n",
        "    plt.title('Computation Profile Across Trials')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('computation_profile.png')\n",
        "    plt.close()\n",
        "\n",
        "    return train_times, cpu_usages, mem_usages, gpu_usages\n",
        "\n",
        "# 6. Error Analysis\n",
        "def plot_error_analysis(model, test_loader):\n",
        "    \"\"\"Analyze model errors with confusion matrix and class metrics\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    class_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Class-wise accuracy\n",
        "    class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(class_names, class_acc, color='skyblue')\n",
        "    plt.axhline(np.mean(class_acc), color='r', linestyle='--',\n",
        "                label=f'Mean Accuracy: {np.mean(class_acc):.4f}')\n",
        "    plt.ylim(0.7, 1.0)\n",
        "    plt.title('Class-wise Accuracy')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('class_accuracy.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Loss distribution (sample level)\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = nn.CrossEntropyLoss(reduction='none')(outputs, labels)\n",
        "            losses.extend(loss.cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(losses, bins=50, color='purple', alpha=0.7)\n",
        "    plt.axvline(np.mean(losses), color='r', linestyle='--',\n",
        "                label=f'Mean Loss: {np.mean(losses):.4f}')\n",
        "    plt.title('Loss Distribution Across Test Samples')\n",
        "    plt.xlabel('Loss')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('loss_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    return cm, class_acc, losses\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Run after your optimization code\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "\n",
        "    # 1. Convergence Progression\n",
        "    plot_convergence(results)\n",
        "    print(\"Generated Convergence Progression plots\")\n",
        "\n",
        "    # 2. Hyperparameter Evolution\n",
        "    plot_hyperparameter_evolution(results)\n",
        "    print(\"Generated Hyperparameter Evolution plots\")\n",
        "\n",
        "    # 3. Search Space Exploration\n",
        "    plot_search_space_exploration(results)\n",
        "    print(\"Generated Search Space Exploration plots\")\n",
        "\n",
        "    # 4. Training Dynamics (for best model)\n",
        "    # Prepare dataloaders for training dynamics\n",
        "    batch_size = int(best_config['batch_size'])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create a new model for training dynamics\n",
        "    model_dynamics = FashionCNN().to(device)\n",
        "    plot_training_dynamics(model_dynamics, train_loader, val_loader)\n",
        "    print(\"Generated Training Dynamics plot\")\n",
        "\n",
        "    # 5. Computation Profile (requires modified training loop)\n",
        "    # We need to re-run the optimization with tracking\n",
        "    print(\"Tracking computation profile (this will re-run trials)...\")\n",
        "    # Save previous results\n",
        "    original_results = results.copy()\n",
        "    results = []\n",
        "    track_computation_profile()\n",
        "    # Restore original results\n",
        "    results = original_results\n",
        "    print(\"Generated Computation Profile plot\")\n",
        "\n",
        "    # 6. Error Analysis (on final model)\n",
        "    plot_error_analysis(final_model, test_loader)\n",
        "    print(\"Generated Error Analysis plots\")\n",
        "\n",
        "    print(\"\\nAll visualizations saved to current directory!\")"
      ],
      "metadata": {
        "id": "HIOpr83BI-RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing CNN hyperparameters using LBFGS**\n",
        "\n"
      ],
      "metadata": {
        "id": "r1EgZFic1a1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# plotting libs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pandas.plotting import parallel_coordinates\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import plotly.express as px\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Set seeds & device ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Data Prep ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "full_train = torchvision.datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "train_ds, val_ds = random_split(full_train, [55000,5000])\n",
        "test_ds = torchvision.datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# subset for search\n",
        "subset_size=5000\n",
        "train_sub = Subset(train_ds, list(range(subset_size)))\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Model ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(), nn.Linear(64*7*7,128), nn.ReLU(), nn.Linear(128,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Train/Eval Helpers ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "def train_lbfgs(model, loader, criterion, optimizer, epochs=3, weight_decay=0.0):\n",
        "    history = {'train_loss':[], 'train_acc':[]}\n",
        "    model.train()\n",
        "    for ep in range(epochs):\n",
        "        total_loss, correct, total = 0.0,0,0\n",
        "        for imgs, labs in loader:\n",
        "            imgs, labs = imgs.to(device), labs.to(device)\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                out = model(imgs)\n",
        "                loss = criterion(out,labs)\n",
        "                if weight_decay>0:\n",
        "                    l2 = sum(p.pow(2).sum() for p in model.parameters())\n",
        "                    loss = loss + weight_decay*l2\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            loss = optimizer.step(closure)\n",
        "            total_loss += loss.item()\n",
        "            with torch.no_grad():\n",
        "                preds = model(imgs).argmax(1)\n",
        "                correct += (preds==labs).sum().item()\n",
        "                total += labs.size(0)\n",
        "        history['train_loss'].append(total_loss/len(loader))\n",
        "        history['train_acc'].append(correct/total)\n",
        "        print(f\"Epoch {ep+1} completed.\")\n",
        "    return history\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    tot_loss, correct, total = 0.0,0,0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labs in loader:\n",
        "            imgs, labs = imgs.to(device), labs.to(device)\n",
        "            out = model(imgs)\n",
        "            tot_loss += criterion(out,labs).item()\n",
        "            preds = out.argmax(1)\n",
        "            correct += (preds==labs).sum().item()\n",
        "            total += labs.size(0)\n",
        "    return tot_loss/len(loader), correct/total\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Hyperparameter Search ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "search_space = {\n",
        "    'lr':          lambda: 10**np.random.uniform(-2,0),\n",
        "    'batch_size':  lambda: random.choice([16,32,64]),\n",
        "    'weight_decay':lambda: np.random.uniform(0,1e-2)\n",
        "}\n",
        "\n",
        "results = []\n",
        "best_acc,best_cfg,best_trial=0, None, -1\n",
        "num_trials=20\n",
        "\n",
        "for t in range(num_trials):\n",
        "    cfg = {k:fn() for k,fn in search_space.items()}\n",
        "    bs,wd,lr = int(cfg['batch_size']), cfg['weight_decay'], cfg['lr']\n",
        "    train_loader = DataLoader(train_sub, batch_size=bs, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=bs, shuffle=False)\n",
        "\n",
        "    model = FashionCNN().to(device)\n",
        "    optimizer = optim.LBFGS(\n",
        "        model.parameters(), lr=lr, max_iter=5,\n",
        "        history_size=10, line_search_fn='strong_wolfe'\n",
        "    )\n",
        "\n",
        "    start=time.time()\n",
        "    _ = train_lbfgs(model, train_loader, criterion, optimizer, epochs=3, weight_decay=wd)\n",
        "    val_loss,val_acc = evaluate(model, val_loader, criterion)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    results.append((cfg, val_loss, val_acc, elapsed))\n",
        "    print(f\"Trial {t+1}/{num_trials} ‚Äî Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}, Time: {elapsed:.1f}s, CFG: {cfg}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc,best_cfg,best_trial = val_acc,cfg,t+1\n",
        "\n",
        "print(f\"\\nüîç Best Hyperparameter Found at trial {best_trial}: {best_cfg}\")\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Final Retrain & Dynamics ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "full_loader = DataLoader(full_train, batch_size=int(best_cfg['batch_size']), shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=int(best_cfg['batch_size']), shuffle=False)\n",
        "\n",
        "model = FashionCNN().to(device)\n",
        "optimizer = optim.LBFGS(\n",
        "    model.parameters(), lr=best_cfg['lr'],\n",
        "    max_iter=5, history_size=10, line_search_fn='strong_wolfe'\n",
        ")\n",
        "train_dyn = train_lbfgs(model, full_loader, criterion, optimizer, epochs=5, weight_decay=best_cfg['weight_decay'])\n",
        "test_loss,test_acc = evaluate(model, test_loader, criterion)\n",
        "print(f\"\\nTest Loss: {test_loss:.6f}, Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî 1. Convergence Progression ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "df_conv = pd.DataFrame([\n",
        "    {'trial': i+1, 'val_acc': acc, 'val_loss': loss, 'time': tm}\n",
        "    for i, (_, loss, acc, tm) in enumerate(results)\n",
        "])\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "sns.lineplot(data=df_conv, x='trial', y='val_acc', marker='o')\n",
        "plt.title(\"Val Acc per Trial\")\n",
        "plt.subplot(1,2,2)\n",
        "sns.lineplot(data=df_conv, x='trial', y='val_loss', marker='o', color='r')\n",
        "plt.title(\"Val Loss per Trial\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî 2. Hyperparameter Evolution ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "df_hp = df_conv.copy()\n",
        "for i,(cfg,_,_,_) in enumerate(results):\n",
        "    for k in ['lr','batch_size','weight_decay']:\n",
        "        df_hp.loc[i,k] = cfg[k]\n",
        "df_pc = df_hp[['lr','batch_size','weight_decay','val_acc']].copy()\n",
        "df_pc['idx'] = df_pc.index.astype(str)\n",
        "plt.figure(figsize=(8,4))\n",
        "parallel_coordinates(df_pc.rename(columns={'idx':'trial'}), 'trial',\n",
        "                     cols=['lr','batch_size','weight_decay','val_acc'],\n",
        "                     color=plt.cm.viridis(np.linspace(0,1,len(df_pc))))\n",
        "plt.xticks(rotation=45); plt.title(\"Hyperparameter Evolution\"); plt.show()\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî 3. Search Space Exploration ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "X = df_hp[['lr','batch_size','weight_decay']].astype(float)\n",
        "Xp = StandardScaler().fit_transform(X)\n",
        "pcs = PCA(3).fit_transform(Xp)\n",
        "df_pca = pd.DataFrame(pcs,columns=['PC1','PC2','PC3'])\n",
        "df_pca['val_acc'] = df_hp['val_acc']\n",
        "\n",
        "fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3',\n",
        "                    color='val_acc', size='val_acc',\n",
        "                    title=\"Search Space Exploration (PCA)\")\n",
        "fig.show()\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî 4. Training Dynamics ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "epochs = list(range(1,len(train_dyn['train_loss'])+1))\n",
        "fig, ax1 = plt.subplots(figsize=(8,4))\n",
        "ax1.plot(epochs, train_dyn['train_loss'], 'b-o', label='Train Loss')\n",
        "ax1.plot(epochs, train_dyn['val_loss']  if 'val_loss' in train_dyn else [None]*len(epochs),\n",
        "         'b--s', label='Val Loss')\n",
        "ax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\", color='b')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, train_dyn['train_acc'], 'g-o', label='Train Acc')\n",
        "ax2.plot(epochs, train_dyn.get('val_acc',[None]*len(epochs)), 'g--s', label='Val Acc')\n",
        "ax2.set_ylabel(\"Accuracy\", color='g')\n",
        "plt.title(\"Train vs Val Dynamics\"); fig.legend(loc='upper right'); plt.show()\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî 5. Computation Profile ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.barplot(x='trial',y='time',data=df_conv,color='skyblue')\n",
        "plt.title(\"Evaluation Time per Trial\"); plt.show()\n",
        "\n",
        "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî 6. Error Analysis ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "# Confusion Matrix\n",
        "all_preds,all_lbls=[],[]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for ims,lbs in test_loader:\n",
        "        ims,lbs=ims.to(device),lbs.to(device)\n",
        "        out=model(ims).argmax(1)\n",
        "        all_preds+=out.cpu().tolist()\n",
        "        all_lbls+=lbs.cpu().tolist()\n",
        "\n",
        "cm = confusion_matrix(all_lbls,all_preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=test_ds.classes)\n",
        "fig,ax=plt.subplots(figsize=(5,5))\n",
        "disp.plot(ax=ax,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\"); plt.show()\n",
        "# Class-wise accuracy\n",
        "accs = cm.diagonal()/cm.sum(axis=1)\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.barplot(x=list(range(10)), y=accs, palette='viridis')\n",
        "plt.title(\"Per-Class Accuracy\"); plt.ylim(0,1); plt.show()"
      ],
      "metadata": {
        "id": "XXfpjguMvFdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing CNN hyperparameters using GA and PSO**"
      ],
      "metadata": {
        "id": "pgY_dVhGE8y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline CNN"
      ],
      "metadata": {
        "id": "o6m4g1XLu437"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports & Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Step 2: Data Loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ7yOBC_OUcJ",
        "outputId": "a33742c1-35fd-4b19-842b-1bc2c1d7caf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define CNN Model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # output: (32, 28, 28)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: (32, 14, 14)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # output: (64, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: (64, 7, 7)\n",
        "\n",
        "            nn.Flatten(),  # output: (64*7*7,)\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = FashionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ZfFBrqU8uR3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loss_total += loss.item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    avg_loss = loss_total / len(dataloader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "Niy633sGuaxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJfdeGYGubs_",
        "outputId": "97ab6bbc-076d-422d-d37e-7e28aa5a0e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 408.0778, Accuracy: 84.32%\n",
            "Epoch [2/5], Loss: 263.7032, Accuracy: 89.76%\n",
            "Epoch [3/5], Loss: 218.7325, Accuracy: 91.42%\n",
            "Epoch [4/5], Loss: 189.3577, Accuracy: 92.58%\n",
            "Epoch [5/5], Loss: 167.2148, Accuracy: 93.36%\n",
            "Test Loss: 0.2429, Test Accuracy: 91.36%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24288778877846753, 91.36)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GA on CNN  "
      ],
      "metadata": {
        "id": "gC2Awl1JvD5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# GPU Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "val_size = 5000\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [len(train_dataset)-val_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "# CNN Model Builder\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, conv1_out, conv2_out, fc_units):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten_size = (conv2_out * 7 * 7)\n",
        "        self.fc1 = nn.Linear(self.flatten_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# GA Hyperparameters\n",
        "POP_SIZE = 20\n",
        "GENERATIONS = 10\n",
        "TOP_K = 4\n",
        "\n",
        "param_bounds = {\n",
        "    \"lr\": [1e-4, 1e-2],\n",
        "    \"conv1_out\": [16, 64],\n",
        "    \"conv2_out\": [32, 128],\n",
        "    \"fc_units\": [64, 256],\n",
        "}\n",
        "\n",
        "def random_gene():\n",
        "    return {\n",
        "        \"lr\": round(10 ** random.uniform(np.log10(param_bounds['lr'][0]), np.log10(param_bounds['lr'][1])), 5),\n",
        "        \"conv1_out\": random.randint(*param_bounds['conv1_out']),\n",
        "        \"conv2_out\": random.randint(*param_bounds['conv2_out']),\n",
        "        \"fc_units\": random.randint(*param_bounds['fc_units'])\n",
        "    }\n",
        "\n",
        "def mutate(gene):\n",
        "    mutated = deepcopy(gene)\n",
        "    key = random.choice(list(param_bounds.keys()))\n",
        "    return random_gene() if random.random() < 0.3 else gene\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    child = {}\n",
        "    for key in param_bounds:\n",
        "        child[key] = parent1[key] if random.random() < 0.5 else parent2[key]\n",
        "    return child\n",
        "\n",
        "# Fitness Evaluation\n",
        "def evaluate(gene):\n",
        "    model = CNN(gene['conv1_out'], gene['conv2_out'], gene['fc_units']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=gene['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1):  # train only 1 epoch\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    return val_loss, accuracy\n",
        "\n",
        "# Genetic Algorithm Core\n",
        "def run_ga():\n",
        "    population = [random_gene() for _ in range(POP_SIZE)]\n",
        "\n",
        "    # Create history storage\n",
        "    ga_history = []\n",
        "\n",
        "    best_overall = None\n",
        "    best_loss = float('inf')\n",
        "    best_acc = 0.0\n",
        "    best_gen = -1\n",
        "\n",
        "    for gen in range(GENERATIONS):\n",
        "        # Evaluate all individuals\n",
        "        scored = []\n",
        "        for gene in population:\n",
        "            loss, acc = evaluate(gene)\n",
        "            scored.append((gene, loss, acc))  # <-- MODIFIED TO STORE TUPLE\n",
        "\n",
        "        scored.sort(key=lambda x: x[1])  # sort by loss\n",
        "\n",
        "        best_gene, loss, acc = scored[0]\n",
        "\n",
        "        # Track global best\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_acc = acc\n",
        "            best_overall = best_gene\n",
        "            best_gen = gen\n",
        "\n",
        "        print(f\"Gen {gen}: Best Loss = {loss:.4f}, Acc = {acc:.4f}\")\n",
        "        print(f\"         Best Hyperparams @ Gen {gen}: {best_gene}\")\n",
        "\n",
        "        # ======== STORE GENERATION HISTORY ======== <-- ADD THIS BLOCK\n",
        "        gen_data = {\n",
        "            'generation': gen,\n",
        "            'population': [gene for gene, _, _ in scored],\n",
        "            'fitness': [(loss, acc) for _, loss, acc in scored],\n",
        "            'best_gene': best_gene,\n",
        "            'best_loss': loss,\n",
        "            'best_acc': acc\n",
        "        }\n",
        "        ga_history.append(gen_data)\n",
        "        # ======== END HISTORY STORAGE ========\n",
        "\n",
        "        # Genetic operations\n",
        "        top_genes = [x[0] for x in scored[:TOP_K]]\n",
        "        new_pop = top_genes.copy()\n",
        "\n",
        "        while len(new_pop) < POP_SIZE:\n",
        "            p1, p2 = random.sample(top_genes, 2)\n",
        "            child = crossover(p1, p2)\n",
        "            child = mutate(child)\n",
        "            new_pop.append(child)\n",
        "\n",
        "        population = new_pop\n",
        "\n",
        "    print(f\"\\n‚úÖ Best hyperparameters found at generation {best_gen}: {best_overall}\")\n",
        "    print(f\"   Final Best Loss = {best_loss:.4f}, Accuracy = {best_acc:.4f}\")\n",
        "    return best_overall, ga_history\n",
        "\n",
        "# Run GA\n",
        "best_hyperparams, ga_history = run_ga()\n",
        "print(\"\\nBest hyperparameters found:\", best_hyperparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "7OT1FPIeuh4j",
        "outputId": "0663b9ad-3827-4a27-8515-2ee91e41d9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-4260260563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# Run GA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mga_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBest hyperparameters found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-4260260563.py\u001b[0m in \u001b[0;36mrun_ga\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mscored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgene\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mscored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- MODIFIED TO STORE TUPLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-4260260563.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(gene)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Official Test Dataset\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Evaluate Best Hyperparams on Test Set\n",
        "def train_and_test(hparams, epochs=5):\n",
        "    model = CNN(hparams['conv1_out'], hparams['conv2_out'], hparams['fc_units']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"\\nüîÅ Training final model for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    test_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f\"\\nüìä Final Evaluation on Test Set:\")\n",
        "    print(f\"   Test Loss     = {test_loss:.4f}\")\n",
        "    print(f\"   Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "train_and_test(best_hyperparams)"
      ],
      "metadata": {
        "id": "T5ta--z4_sZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PSO on CNN"
      ],
      "metadata": {
        "id": "7AOpLOmo02Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# ------------------------------\n",
        "# Device Setup\n",
        "# ------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------------------\n",
        "# Dataset (Subset for Speed)\n",
        "# ------------------------------\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "val_size = 5000\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [len(train_dataset) - val_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test Set (Official Evaluation)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ------------------------------\n",
        "# CNN Definition\n",
        "# ------------------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, conv1_out, conv2_out, fc_units):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten_size = conv2_out * 7 * 7\n",
        "        self.fc1 = nn.Linear(self.flatten_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ------------------------------\n",
        "# Hyperparameter Bounds\n",
        "# ------------------------------\n",
        "param_bounds = {\n",
        "    'lr': [1e-4, 1e-2],\n",
        "    'conv1_out': [16, 64],\n",
        "    'conv2_out': [32, 128],\n",
        "    'fc_units': [64, 256]\n",
        "}\n",
        "\n",
        "# ------------------------------\n",
        "# Fitness Function\n",
        "# ------------------------------\n",
        "def evaluate(hparams):\n",
        "    model = CNN(int(hparams['conv1_out']), int(hparams['conv2_out']), int(hparams['fc_units'])).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx >= 9:  # train on 10 mini-batches\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    return val_loss, accuracy\n",
        "\n",
        "# ------------------------------\n",
        "# PSO Core\n",
        "# ------------------------------\n",
        "def run_pso(particles=10, generations=20, w=0.5, c1=1.5, c2=1.5):\n",
        "    def init_particle():\n",
        "        return {\n",
        "            'lr': 10 ** random.uniform(np.log10(param_bounds['lr'][0]), np.log10(param_bounds['lr'][1])),\n",
        "            'conv1_out': random.randint(*param_bounds['conv1_out']),\n",
        "            'conv2_out': random.randint(*param_bounds['conv2_out']),\n",
        "            'fc_units': random.randint(*param_bounds['fc_units'])\n",
        "        }\n",
        "\n",
        "    def clip(hp):\n",
        "        hp['lr'] = float(np.clip(hp['lr'], *param_bounds['lr']))\n",
        "        hp['conv1_out'] = int(np.clip(round(hp['conv1_out']), *param_bounds['conv1_out']))\n",
        "        hp['conv2_out'] = int(np.clip(round(hp['conv2_out']), *param_bounds['conv2_out']))\n",
        "        hp['fc_units'] = int(np.clip(round(hp['fc_units']), *param_bounds['fc_units']))\n",
        "        return hp\n",
        "\n",
        "    population = [init_particle() for _ in range(particles)]\n",
        "    velocity = [{k: 0.0 for k in p} for p in population]\n",
        "\n",
        "    personal_best = deepcopy(population)\n",
        "    personal_best_scores = []\n",
        "    personal_best_accs = []\n",
        "\n",
        "    for p in personal_best:\n",
        "        loss, acc = evaluate(p)\n",
        "        personal_best_scores.append(loss)\n",
        "        personal_best_accs.append(acc)\n",
        "\n",
        "    global_best_idx = np.argmin(personal_best_scores)\n",
        "    global_best = deepcopy(personal_best[global_best_idx])\n",
        "    global_best_loss = personal_best_scores[global_best_idx]\n",
        "    global_best_acc = personal_best_accs[global_best_idx]\n",
        "\n",
        "    pso_history = []\n",
        "\n",
        "    for gen in range(generations):\n",
        "        for i in range(particles):\n",
        "            new_vel = {}\n",
        "            new_pos = {}\n",
        "            for key in population[i]:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                vel = (w * velocity[i][key]\n",
        "                       + c1 * r1 * (personal_best[i][key] - population[i][key])\n",
        "                       + c2 * r2 * (global_best[key] - population[i][key]))\n",
        "                new_vel[key] = vel\n",
        "                new_pos[key] = population[i][key] + vel\n",
        "            velocity[i] = new_vel\n",
        "            new_pos = clip(new_pos)\n",
        "            population[i] = new_pos\n",
        "\n",
        "            loss, acc = evaluate(new_pos)\n",
        "            if loss < personal_best_scores[i]:\n",
        "                personal_best[i] = deepcopy(new_pos)\n",
        "                personal_best_scores[i] = loss\n",
        "                personal_best_accs[i] = acc\n",
        "\n",
        "        global_best_idx = np.argmin(personal_best_scores)\n",
        "        global_best = deepcopy(personal_best[global_best_idx])\n",
        "        global_best_loss = personal_best_scores[global_best_idx]\n",
        "        global_best_acc = personal_best_accs[global_best_idx]\n",
        "\n",
        "        print(f\"Gen {gen}: Best Loss = {global_best_loss:.4f}, Acc = {global_best_acc:.4f}\")\n",
        "\n",
        "        # ======== STORE ITERATION HISTORY ======== <-- ADD THIS BLOCK\n",
        "        gen_data = {\n",
        "            'iteration': gen,\n",
        "            'swarm': deepcopy(population),\n",
        "            'fitness': [(personal_best_scores[i], personal_best_accs[i])\n",
        "                        for i in range(len(population))],\n",
        "            'global_best': deepcopy(global_best),\n",
        "            'global_loss': global_best_loss,\n",
        "            'global_acc': global_best_acc\n",
        "        }\n",
        "        pso_history.append(gen_data)\n",
        "        # ======== END HISTORY STORAGE ========\n",
        "\n",
        "    return global_best, pso_history\n",
        "\n",
        "# ------------------------------\n",
        "# Run PSO\n",
        "# ------------------------------\n",
        "best_pso_hyperparams, pso_history = run_pso()\n",
        "print(\"\\nBest hyperparameters from PSO:\", best_pso_hyperparams)\n",
        "\n",
        "# ------------------------------\n",
        "# Final Full Training & Test Evaluation\n",
        "# ------------------------------\n",
        "def train_and_test(hparams, epochs=5):\n",
        "    model = CNN(hparams['conv1_out'], hparams['conv2_out'], hparams['fc_units']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "train_and_test(best_pso_hyperparams)"
      ],
      "metadata": {
        "id": "YaPSAbMk04U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plotting of GA vs PSO**"
      ],
      "metadata": {
        "id": "onRIk2DOg9vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================================\n",
        "# PLOTTING FUNCTIONS\n",
        "# ==================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from pandas.plotting import parallel_coordinates\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Normalization helper\n",
        "def normalize_param(value, param_name):\n",
        "    bounds = param_bounds[param_name]\n",
        "    if param_name == 'lr':\n",
        "        log_val = np.log10(value)\n",
        "        log_min = np.log10(bounds[0])\n",
        "        log_max = np.log10(bounds[1])\n",
        "        return (log_val - log_min) / (log_max - log_min)\n",
        "    return (value - bounds[0]) / (bounds[1] - bounds[0])"
      ],
      "metadata": {
        "id": "e8PMtfcsucH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CONVERGENCE PLOT\n",
        "def plot_convergence(ga_history, pso_history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # GA data\n",
        "    ga_loss = [gen['best_loss'] for gen in ga_history]\n",
        "    ga_acc = [gen['best_acc'] for gen in ga_history]\n",
        "\n",
        "    # PSO data\n",
        "    pso_loss = [gen['global_loss'] for gen in pso_history]\n",
        "    pso_acc = [gen['global_acc'] for gen in pso_history]\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(ga_loss, 'b-o', label='GA')\n",
        "    plt.plot(pso_loss, 'r-s', label='PSO')\n",
        "    plt.xlabel('Generation/Iteration')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.title('Loss Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(ga_acc, 'b-o', label='GA')\n",
        "    plt.plot(pso_acc, 'r-s', label='PSO')\n",
        "    plt.xlabel('Generation/Iteration')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.title('Accuracy Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('convergence.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_convergence(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "fKre15qIudNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 3D TRAJECTORY ANIMATION\n",
        "def create_3d_animation(ga_history, pso_history):\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Prepare data containers\n",
        "    ga_points = []\n",
        "    pso_points = []\n",
        "\n",
        "    # Create animation function\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "\n",
        "        # GA points\n",
        "        for gen in range(min(frame+1, len(ga_history))):\n",
        "            for gene in ga_history[gen]['population']:\n",
        "                x = normalize_param(gene['lr'], 'lr')\n",
        "                y = normalize_param(gene['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(gene['conv2_out'], 'conv2_out')\n",
        "                ga_points.append((x, y, z))\n",
        "\n",
        "        # PSO points\n",
        "        for iter in range(min(frame+1, len(pso_history))):\n",
        "            for particle in pso_history[iter]['swarm']:\n",
        "                x = normalize_param(particle['lr'], 'lr')\n",
        "                y = normalize_param(particle['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(particle['conv2_out'], 'conv2_out')\n",
        "                pso_points.append((x, y, z))\n",
        "\n",
        "        # Convert to arrays\n",
        "        if ga_points:\n",
        "            ga_arr = np.array(ga_points)\n",
        "            ax.scatter(ga_arr[:,0], ga_arr[:,1], ga_arr[:,2],\n",
        "                      c='blue', alpha=0.3, s=10, label='GA')\n",
        "\n",
        "        if pso_points:\n",
        "            pso_arr = np.array(pso_points)\n",
        "            ax.scatter(pso_arr[:,0], pso_arr[:,1], pso_arr[:,2],\n",
        "                      c='red', alpha=0.3, s=10, marker='s', label='PSO')\n",
        "\n",
        "        # Plot best trajectories\n",
        "        if ga_history:\n",
        "            ga_best = []\n",
        "            for gen in range(min(frame+1, len(ga_history))):\n",
        "                gene = ga_history[gen]['best_gene']\n",
        "                x = normalize_param(gene['lr'], 'lr')\n",
        "                y = normalize_param(gene['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(gene['conv2_out'], 'conv2_out')\n",
        "                ga_best.append((x, y, z))\n",
        "            ga_best_arr = np.array(ga_best)\n",
        "            ax.plot(ga_best_arr[:,0], ga_best_arr[:,1], ga_best_arr[:,2],\n",
        "                   'b-', linewidth=2, label='GA Best')\n",
        "            ax.scatter(ga_best_arr[-1,0], ga_best_arr[-1,1], ga_best_arr[-1,2],\n",
        "                      c='gold', s=100, edgecolor='black', label='Current Best')\n",
        "\n",
        "        if pso_history:\n",
        "            pso_best = []\n",
        "            for iter in range(min(frame+1, len(pso_history))):\n",
        "                particle = pso_history[iter]['global_best']\n",
        "                x = normalize_param(particle['lr'], 'lr')\n",
        "                y = normalize_param(particle['conv1_out'], 'conv1_out')\n",
        "                z = normalize_param(particle['conv2_out'], 'conv2_out')\n",
        "                pso_best.append((x, y, z))\n",
        "            pso_best_arr = np.array(pso_best)\n",
        "            ax.plot(pso_best_arr[:,0], pso_best_arr[:,1], pso_best_arr[:,2],\n",
        "                   'r-', linewidth=2, label='PSO Best')\n",
        "\n",
        "        # Set labels and title\n",
        "        ax.set_xlabel('log10(Learning Rate)')\n",
        "        ax.set_ylabel('conv1_out')\n",
        "        ax.set_zlabel('conv2_out')\n",
        "        ax.set_title(f'Hyperparameter Optimization Trajectories\\nGeneration/Iteration: {frame}')\n",
        "        ax.legend()\n",
        "\n",
        "        # Set viewing angle\n",
        "        ax.view_init(30, frame * 2)\n",
        "\n",
        "    # Create animation\n",
        "    ani = FuncAnimation(fig, update, frames=max(len(ga_history), len(pso_history)),\n",
        "                        interval=500, blit=False)\n",
        "\n",
        "    # Save animation\n",
        "    ani.save('optimization_trajectory.mp4', writer='ffmpeg', fps=2, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "create_3d_animation(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "obPtWePVuqun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. PARALLEL COORDINATES PLOT\n",
        "def plot_parallel_coordinates(ga_history, pso_history):\n",
        "    data = []\n",
        "\n",
        "    # Collect GA data\n",
        "    for gen in ga_history:\n",
        "        for gene, (loss, acc) in zip(gen['population'], gen['fitness']):\n",
        "            row = gene.copy()\n",
        "            row['loss'] = loss\n",
        "            row['accuracy'] = acc\n",
        "            row['method'] = 'GA'\n",
        "            data.append(row)\n",
        "\n",
        "    # Collect PSO data\n",
        "    for gen in pso_history:\n",
        "        for particle, (loss, acc) in zip(gen['swarm'], gen['fitness']):\n",
        "            row = particle.copy()\n",
        "            row['loss'] = loss\n",
        "            row['accuracy'] = acc\n",
        "            row['method'] = 'PSO'\n",
        "            data.append(row)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Normalize parameters for better visualization\n",
        "    for param in param_bounds.keys():\n",
        "        if param == 'lr':\n",
        "            df['log_lr'] = np.log10(df['lr'])\n",
        "            min_val = np.log10(param_bounds['lr'][0])\n",
        "            max_val = np.log10(param_bounds['lr'][1])\n",
        "            df['norm_lr'] = (df['log_lr'] - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            min_val = param_bounds[param][0]\n",
        "            max_val = param_bounds[param][1]\n",
        "            df[f'norm_{param}'] = (df[param] - min_val) / (max_val - min_val)\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    parallel_coordinates(\n",
        "        df[['method', 'norm_lr', 'norm_conv1_out', 'norm_conv2_out', 'norm_fc_units', 'accuracy']],\n",
        "        'method',\n",
        "        color=['blue', 'red'],\n",
        "        alpha=0.1\n",
        "    )\n",
        "    plt.title('Hyperparameter Space Exploration')\n",
        "    plt.xlabel('Parameters (Normalized)')\n",
        "    plt.ylabel('Value')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig('parallel_coordinates.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_parallel_coordinates(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "ZEKwtBZiurnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. HYPERPARAMETER EVOLUTION\n",
        "def plot_hyperparameter_evolution(ga_history, pso_history):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    params = ['lr', 'conv1_out', 'conv2_out', 'fc_units']\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "        ax = axs[i//2, i%2]\n",
        "\n",
        "        # GA evolution\n",
        "        ga_vals = []\n",
        "        for gen in ga_history:\n",
        "            gen_vals = [gene[param] for gene in gen['population']]\n",
        "            ga_vals.append(gen_vals)\n",
        "        ga_vals = np.array(ga_vals)\n",
        "\n",
        "        # Plot GA\n",
        "        for j in range(ga_vals.shape[1]):\n",
        "            ax.plot(ga_vals[:, j], 'b-', alpha=0.1)\n",
        "        ax.plot(np.median(ga_vals, axis=1), 'b-o', linewidth=2, label='GA Median')\n",
        "\n",
        "        # PSO evolution\n",
        "        pso_vals = []\n",
        "        for gen in pso_history:\n",
        "            gen_vals = [particle[param] for particle in gen['swarm']]\n",
        "            pso_vals.append(gen_vals)\n",
        "        pso_vals = np.array(pso_vals)\n",
        "\n",
        "        # Plot PSO\n",
        "        for j in range(pso_vals.shape[1]):\n",
        "            ax.plot(pso_vals[:, j], 'r-', alpha=0.1)\n",
        "        ax.plot(np.median(pso_vals, axis=1), 'r-s', linewidth=2, label='PSO Median')\n",
        "\n",
        "        # Formatting\n",
        "        ax.set_xlabel('Generation/Iteration')\n",
        "        ax.set_ylabel(param)\n",
        "        if param == 'lr':\n",
        "            ax.set_yscale('log')\n",
        "        ax.set_title(f'{param} Evolution')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hyperparameter_evolution.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_hyperparameter_evolution(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "tdnFwGf6utnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. PERFORMANCE HEATMAP\n",
        "def plot_performance_heatmap(ga_history, pso_history):\n",
        "    # Prepare data\n",
        "    all_data = []\n",
        "\n",
        "    # GA data\n",
        "    for gen in ga_history:\n",
        "        for gene, (loss, acc) in zip(gen['population'], gen['fitness']):\n",
        "            all_data.append({\n",
        "                'method': 'GA',\n",
        "                'conv1_out': gene['conv1_out'],\n",
        "                'conv2_out': gene['conv2_out'],\n",
        "                'accuracy': acc\n",
        "            })\n",
        "\n",
        "    # PSO data\n",
        "    for gen in pso_history:\n",
        "        for particle, (loss, acc) in zip(gen['swarm'], gen['fitness']):\n",
        "            all_data.append({\n",
        "                'method': 'PSO',\n",
        "                'conv1_out': particle['conv1_out'],\n",
        "                'conv2_out': particle['conv2_out'],\n",
        "                'accuracy': acc\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Create pivot tables\n",
        "    ga_pivot = df[df['method']=='GA'].pivot_table(\n",
        "        index='conv1_out', columns='conv2_out', values='accuracy', aggfunc='mean'\n",
        "    )\n",
        "    pso_pivot = df[df['method']=='PSO'].pivot_table(\n",
        "        index='conv1_out', columns='conv2_out', values='accuracy', aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    # Plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
        "\n",
        "    # GA heatmap\n",
        "    sns.heatmap(ga_pivot, annot=True, fmt=\".3f\", cmap=\"Blues\", ax=ax1)\n",
        "    ax1.set_title('GA Accuracy Heatmap')\n",
        "    ax1.set_xlabel('conv2_out')\n",
        "    ax1.set_ylabel('conv1_out')\n",
        "\n",
        "    # PSO heatmap\n",
        "    sns.heatmap(pso_pivot, annot=True, fmt=\".3f\", cmap=\"Reds\", ax=ax2)\n",
        "    ax2.set_title('PSO Accuracy Heatmap')\n",
        "    ax2.set_xlabel('conv2_out')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_heatmap.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_performance_heatmap(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "sQJ2rpdDuv8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. INTERACTIVE 3D PLOT (PLOTLY)\n",
        "def create_interactive_3d(ga_history, pso_history):\n",
        "    # Prepare data\n",
        "    ga_data = []\n",
        "    pso_data = []\n",
        "\n",
        "    # GA points\n",
        "    for gen in ga_history:\n",
        "        for gene, (loss, acc) in zip(gen['population'], gen['fitness']):\n",
        "            ga_data.append({\n",
        "                'x': np.log10(gene['lr']),\n",
        "                'y': gene['conv1_out'],\n",
        "                'z': gene['conv2_out'],\n",
        "                'accuracy': acc,\n",
        "                'fc_units': gene['fc_units'],\n",
        "                'generation': gen['generation']\n",
        "            })\n",
        "\n",
        "    # PSO points\n",
        "    for gen in pso_history:\n",
        "        for particle, (loss, acc) in zip(gen['swarm'], gen['fitness']):\n",
        "            pso_data.append({\n",
        "                'x': np.log10(particle['lr']),\n",
        "                'y': particle['conv1_out'],\n",
        "                'z': particle['conv2_out'],\n",
        "                'accuracy': acc,\n",
        "                'fc_units': particle['fc_units'],\n",
        "                'iteration': gen['iteration']\n",
        "            })\n",
        "\n",
        "    # Create GA trace\n",
        "    ga_df = pd.DataFrame(ga_data)\n",
        "    ga_trace = go.Scatter3d(\n",
        "        x=ga_df['x'],\n",
        "        y=ga_df['y'],\n",
        "        z=ga_df['z'],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=ga_df['fc_units']/50,\n",
        "            color=ga_df['accuracy'],\n",
        "            colorscale='Blues',\n",
        "            opacity=0.7,\n",
        "            colorbar=dict(title='Accuracy')\n",
        "        ),\n",
        "        name='GA',\n",
        "        hovertext=ga_df.apply(lambda row:\n",
        "            f\"Accuracy: {row['accuracy']:.4f}<br>\"\n",
        "            f\"LR: {10**row['x']:.5f}<br>\"\n",
        "            f\"conv1: {row['y']}<br>\"\n",
        "            f\"conv2: {row['z']}<br>\"\n",
        "            f\"fc: {row['fc_units']}<br>\"\n",
        "            f\"Gen: {row['generation']}\", axis=1)\n",
        "    )\n",
        "\n",
        "    # Create PSO trace\n",
        "    pso_df = pd.DataFrame(pso_data)\n",
        "    pso_trace = go.Scatter3d(\n",
        "        x=pso_df['x'],\n",
        "        y=pso_df['y'],\n",
        "        z=pso_df['z'],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=pso_df['fc_units']/50,\n",
        "            color=pso_df['accuracy'],\n",
        "            colorscale='Reds',\n",
        "            opacity=0.7,\n",
        "            colorbar=dict(title='Accuracy')\n",
        "        ),\n",
        "        name='PSO',\n",
        "        hovertext=pso_df.apply(lambda row:\n",
        "            f\"Accuracy: {row['accuracy']:.4f}<br>\"\n",
        "            f\"LR: {10**row['x']:.5f}<br>\"\n",
        "            f\"conv1: {row['y']}<br>\"\n",
        "            f\"conv2: {row['z']}<br>\"\n",
        "            f\"fc: {row['fc_units']}<br>\"\n",
        "            f\"Iter: {row['iteration']}\", axis=1)\n",
        "    )\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure(data=[ga_trace, pso_trace])\n",
        "\n",
        "    # Set layout\n",
        "    fig.update_layout(\n",
        "        title='Hyperparameter Optimization Space',\n",
        "        scene=dict(\n",
        "            xaxis_title='log10(Learning Rate)',\n",
        "            yaxis_title='conv1_out',\n",
        "            zaxis_title='conv2_out',\n",
        "            camera=dict(\n",
        "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
        "            )\n",
        "        ),\n",
        "        width=1200,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "    # Save and show\n",
        "    fig.write_html(\"interactive_optimization.html\")\n",
        "    fig.show()\n",
        "\n",
        "create_interactive_3d(ga_history, pso_history)"
      ],
      "metadata": {
        "id": "t7OFYCPtuygT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}